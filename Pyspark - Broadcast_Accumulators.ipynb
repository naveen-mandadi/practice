{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6e2d70",
   "metadata": {},
   "source": [
    "# Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072c2f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all \\nnodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark \\ndistributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all \n",
    "nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark \n",
    "distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5111c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Pyspark-Examples') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8044930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[5] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "dataRDD = spark.sparkContext.parallelize(data)\n",
    "\n",
    "print(dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48bee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9d87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Broadcasting Variable to all nodes in cluster\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13dd785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = dataRDD.map(lambda x: (x[0],x[1],x[2],state_convert(x[3])))\n",
    "\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lmitations\n",
    "#1.apply hash Algarithm -> HashMap, Cached shared to every nodes, read-only structured\n",
    "#2.some memory in executor, 2GB [1GB- times], 1GB\n",
    "#If broadcasted data size is more than 24 MB, better not to use broadcast variables \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4efaae",
   "metadata": {},
   "source": [
    "# Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37e6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#Useful to get count the elements in every Executors without confusion\n",
    "'''The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations \n",
    "similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation \n",
    "or computative operations.'''\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "\n",
    "def sum123(ele):\n",
    "    return ele = ele +1\n",
    "\n",
    "res= rdd.map(r=>sum123(r))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e6e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
