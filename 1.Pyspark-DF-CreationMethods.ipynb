{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69603792",
   "metadata": {},
   "source": [
    "# Pyspark - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f037cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can manually create a PySpark DataFrame using \\n1. toDF() - Convertng from RDDs to Dataframe\\n2. createDataFrame()  - Creatng dataframe from Collections like list, seq,..etc\\n3. from data sources like TXT, CSV, JSON, ORV, Avro, Parquet, XML formats by reading from HDFS, S3, DBFS, Azure Blob file systems e.t.c.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''You can manually create a PySpark DataFrame using \n",
    "1. toDF() - Convertng from RDDs to Dataframe\n",
    "2. createDataFrame()  - Creatng dataframe from Collections like list, seq,..etc\n",
    "3. from data sources like TXT, CSV, JSON, ORV, Avro, Parquet, XML formats by reading from HDFS, S3, DBFS, Azure Blob file systems e.t.c.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bcd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Pyspark-Examples') \\\n",
    "                    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f720796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114866d4",
   "metadata": {},
   "source": [
    "# 1. Using toDF() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf627c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf4b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0fee9",
   "metadata": {},
   "source": [
    "# 2. Using createDataFrame() from SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc8f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- lang: string (nullable = true)\n",
      " |-- cnt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()\n",
    "\n",
    "dfFromRDD3 = spark.createDataFrame(rdd).toDF('lang', 'cnt')\n",
    "dfFromRDD3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5b7b0",
   "metadata": {},
   "source": [
    "# 3. Create DataFrame with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85581b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa4871",
   "metadata": {},
   "source": [
    "# 4. Create DataFrame from Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa52cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df2 = spark.read.csv(\"/src/resources/file.csv\")\n",
    "#df3 = spark.read.text(\"/src/resources/file.txt\")\n",
    "#df4 = spark.read.json(\"/src/resources/file.json\")\n",
    "\n",
    "'''OR Using below methods'''\n",
    "\n",
    "# orc/csv/avro/parquet/Kafka/jdbc\n",
    "#df5 = spark.read.format('avro').load('/src/resources/file.avsc')  #pass file type as argument orc/csv/avro/parquet/Kafka/jdbc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c7748",
   "metadata": {},
   "source": [
    "# 5. Create Empty Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4281a94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create empty DatFrame with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()\n",
    "\n",
    "#print below empty schema\n",
    "#root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673e815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
